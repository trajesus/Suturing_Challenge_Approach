{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec27f7e-59e2-4f8a-a7fd-f6503249402c",
   "metadata": {},
   "source": [
    "# Suturing Chalange - Task1 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453108d-1df8-4676-aa78-1b9f9c53acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your Libraries \n",
    "import os\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from torch.nn import Sigmoid, ReLU, Softmax, Module, Linear\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn.init import xavier_uniform_, zeros_\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import to_rgba_array\n",
    "from glob import glob\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "from livelossplot import PlotLosses\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import Dataset, DataLoader, ImageDataset, CacheDataset, NumpyReader, CSVDataset\n",
    "\n",
    "from monai.transforms import (\n",
    "    #import necessary transforms\n",
    "    Activations,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CastToTyped,\n",
    "    EnsureType,\n",
    "    LoadImaged,\n",
    "    EnsureTyped,\n",
    "    ToTensor\n",
    ")\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758a7b4-6fb8-4e72-bb68-607900f30efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(TORCH_SEED):\n",
    "    random.seed(TORCH_SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(TORCH_SEED)\n",
    "    np.random.seed(TORCH_SEED)\n",
    "    torch.manual_seed(TORCH_SEED)\n",
    "    torch.cuda.manual_seed_all(TORCH_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fa548-b6ad-40e4-ac69-0a8e70ab97cc",
   "metadata": {},
   "source": [
    "## Step 1 - check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4d53a-6492-4592-a6f4-9dcaeecdbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_base_path = \"./Dataset/Train\"\n",
    "Csv_file_path = f\"{Dataset_base_path}/OSATS.csv\"\n",
    "\n",
    "df = pd.read_csv(Csv_file_path, sep = \";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759e868-0c14-46a0-b4ac-09e6acb1a330",
   "metadata": {},
   "source": [
    "## Step 2 - Feature extraction\n",
    "2 Yolo versions were experimented with:\n",
    "- Yolo v8 - follow step 2.1\n",
    "- Yolo v5 - follow step 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2fb21e-42f6-45ed-bcad-a042644f4a9b",
   "metadata": {},
   "source": [
    "## Step 2.1 - Prepare Yolo model v8 to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2651029-0608-4773-8992-984ae8bafb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  YOLO Functions  #####################\n",
    "# Define hook function\n",
    "def hook_fn(module, input, output):\n",
    "    intermediate_features.append(output)\n",
    "\n",
    "# Define feature extraction function\n",
    "def extract_features(model, img, layer_index = 20): ##Choose the layer that fit your application\n",
    "    global intermediate_features\n",
    "    intermediate_features = []\n",
    "    hook = model.model.model[layer_index].register_forward_hook(hook_fn)\n",
    "    #print(hook)\n",
    "    with torch.no_grad():\n",
    "        model(img)\n",
    "    hook.remove()\n",
    "    return intermediate_features[0]  # Access the first element of the list\n",
    "\n",
    "# Make sure to preprocess the image since the input image must be 640x640x3\n",
    "def preprocess_image(img_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((640, 640)),\n",
    "        transforms.Grayscale(num_output_channels = 3),  # Convert to RGB\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = 0., std = 1.)\n",
    "    ])\n",
    "    if(type(img_path) == type(Path())):\n",
    "        img = Image.open(img_path)\n",
    "    else:\n",
    "        img = Image.fromarray(img_path)\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "# Plot the Features extracted\n",
    "def extract_and_plot_features(img_path, layer_index=20, channel_index=5):\n",
    "    model = YOLO(Path(\"./model/yolov8n.pt\"))\n",
    "    img = preprocess_image(img_path)\n",
    "    features = extract_features(model, img, layer_index)\n",
    "\n",
    "        # Print the shape of the features\n",
    "    print(f\"Features shape for {img_path.name if(type(img_path) == type(Path())) else 'this frame'}: {features.shape}\")\n",
    "\n",
    "        # Plot the features as a heatmap for a specific channel\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(features[0][channel_index].cpu().numpy(), cmap='viridis', annot=False)\n",
    "    plt.title(f'Features for {img_path.name if(type(img_path) == type(Path())) else \"this frame\"} - Layer {layer_index} - Channel {channel_index}')\n",
    "    plt.show()\n",
    "\n",
    "def get_features(img, layer_index = 20):\n",
    "    model = YOLO(Path(\"./model/yolov8n.pt\"))\n",
    "    return extract_features(model, preprocess_image(img), layer_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2c29a-d092-4324-9f31-2302a0da92b7",
   "metadata": {},
   "source": [
    "## Step 2.2 - Prepare Yolo model v5 to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3022c0-60c3-42da-9816-a5dfc21412ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extraction function\n",
    "def get_features_from_layer(model, image_tensor, layer_idx=-2):\n",
    "    \"\"\"\n",
    "    Extract features from a specific layer.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The loaded YOLOv5 model.\n",
    "    - image_tensor: The input image tensor with shape [C, H, W] and batch dimension added.\n",
    "    - layer_idx: Index of the layer from which to extract features. Default is -2, the second to last layer.\n",
    "    \n",
    "    Returns:\n",
    "    - features: The extracted features from the specified layer.\n",
    "    \"\"\"\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Hook to capture the output of the specified layer\n",
    "    features = []\n",
    "    def hook_fn(module, input, output):\n",
    "        features.append(output)\n",
    "    \n",
    "    # Register the hook to the desired layer\n",
    "    handle = list(model.model.model.children())[0][-2].register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Perform a forward pass to get the features\n",
    "    with torch.no_grad():\n",
    "        _ = model(image_tensor)  # Add batch dimension if not present\n",
    "    \n",
    "    # Remove the hook\n",
    "    handle.remove()\n",
    "    \n",
    "    # Return the features\n",
    "    return features[0]\n",
    "\n",
    "# Make sure to preprocess the image since the input image must be 640x640x3\n",
    "def preprocess_image(img_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((320, 320)),\n",
    "        transforms.Grayscale(num_output_channels = 3),  # Convert to RGB\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = 0., std = 1.)\n",
    "    ])\n",
    "    if(type(img_path) == type(Path())):\n",
    "        img = Image.open(img_path)\n",
    "    else:\n",
    "        img = Image.fromarray(img_path)\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "def get_features(img, layer_index=-2):\n",
    "    pre_process_img = preprocess_image(img)\n",
    "    print(f\"Pre processed image shape: {pre_process_img.shape}\")\n",
    "    return get_features_from_layer(model=model, image_tensor=pre_process_img, layer_idx=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c262d-3198-49d5-937f-b9964b673529",
   "metadata": {},
   "source": [
    "## Step 3 - Run trough the videos and extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2016389-9a26-4025-a8d6-a340c5165363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  OpenCV Functions  #####################\n",
    "def process_video(video_name, clip_minutes=5):\n",
    "    cap = cv2.VideoCapture(f'{video_name}.mp4')  # Create a VideoCapture object and read from input file\n",
    "    video_frame_features = []\n",
    "    \n",
    "    # Check if capture opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video stream or file\")\n",
    "        return None\n",
    "\n",
    "    # Get frame rate (frames per second)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(f'Frames per second: {fps} FPS')\n",
    "\n",
    "    # Total frames for the first 5 minutes (clip_minutes * 60 seconds * fps)\n",
    "    max_frames = int(clip_minutes * 60 * fps)\n",
    "    \n",
    "    # Get the total number of frames in the video\n",
    "    frames_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f'Frame count in video: {frames_total}')\n",
    "\n",
    "    # Ensure we don't process more than the total number of frames in the video\n",
    "    frames_to_process = min(max_frames, frames_total)\n",
    "    print(f'Processing {frames_to_process} frames (up to {clip_minutes} minutes)')\n",
    "\n",
    "    # Capture one frame per second (skip frames based on fps)\n",
    "    for frame in range(frames_to_process):\n",
    "        ret, img = cap.read()  # Capture frame-by-frame\n",
    "        if not ret:\n",
    "            break  # Break if we can't read the frame\n",
    "        \n",
    "        if frame % fps == 0:  # Process one frame per second\n",
    "            print(f'Processing frame {frame}/{frames_to_process}')\n",
    "            print(f\"Input image shape: {img.shape}\")\n",
    "            processed_frame = get_features(img).cpu().numpy()  # Process the frame\n",
    "            print(f\"Feature shape: {processed_frame.shape}\")\n",
    "            video_frame_features.append(processed_frame)\n",
    "\n",
    "    # Release the video capture object and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return np.array(video_frame_features)\n",
    "\n",
    "def process_videos():\n",
    "    for i in range(len(video_list)):\n",
    "        video_name = video_list[i]\n",
    "        numpy_file = f\"/notebooks/disk4/Suturing_Challenge/Hand_gesture/features_extracted/{video_name}.npz\"\n",
    "        video_score = np.array(0 if rating_score[i] < 16 else 1 if 15 < rating_score[i] < 24 else 2 if 23 < rating_score[i] < 32 else 3)\n",
    "        if(not os.path.isfile(numpy_file)): # if the video hasn't been already processed\n",
    "            print(f\"Video {i}/{len(video_list)} - {video_name}.mp4 Score: {video_score}\")\n",
    "            video_features = process_video(f\"{Dataset_base_path}/videos/{video_name}\")\n",
    "            np.savez(numpy_file, video_features, video_score) #usar a compressed como abaixo\n",
    "\n",
    "def convert_numpy_files():\n",
    "    all_videos = glob(\"./numpy_files/*.npz\")\n",
    "    for file in all_videos:\n",
    "        old_numpy_file = np.load(file)\n",
    "        new_file = f\"./Dataset/Train/numpy_files/{file.split('/')[-1]}\"\n",
    "        #get video\n",
    "        video = old_numpy_file[\"arr_0\"].reshape( -1, 384, 20, 20)[::3,:,:,:][0:300] #sub sample from 3FPS to 1 and select the first 300 (5min)\n",
    "        #get scores\n",
    "        score = old_numpy_file[\"arr_1\"]\n",
    "        np.savez_compressed(new_file, video = video, score = score)\n",
    "\n",
    "def reduce_numpy(): #apply PCA\n",
    "    numpy_files = glob(\"./Dataset/Train/numpy_files/*.npz\")\n",
    "    reduced_numpy_path = \"./Dataset/Train/numpy_files_reduced\"\n",
    "    pca = PCA()\n",
    "    for i,file in enumerate(numpy_files):\n",
    "        print(f\"File {i+1}/{len(numpy_files)}\")\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "        print(file_name)\n",
    "        numpy_video = np.load(file)[\"video\"]\n",
    "        print(numpy_video.shape)\n",
    "        pca.fit(numpy_video.reshape(300, -1))\n",
    "        features_reduced = pca.transform(numpy_video.reshape(300, -1))\n",
    "        print(f\"Original shape: {numpy_video.reshape(300, -1).shape}\")\n",
    "        print(f\"New shape: {features_reduced.shape}\")\n",
    "        np.savez_compressed(f\"{reduced_numpy_path}/{file_name}\", video = features_reduced)\n",
    "\n",
    "reduce_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a215c-2930-4367-b79f-7d9b7359805f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10782128-48ff-4622-9b0f-229d0432137f",
   "metadata": {},
   "source": [
    "## Step 4 - prepare dataset.csv to be used with data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885ef06-fa97-4214-b46d-6edbed1055ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GRS(score_list, index):\n",
    "    score1 = 0 if score_list[index] < 16 else 1 if 15 < score_list[index] < 24 else 2 if 23 < score_list[index] < 32 else 3\n",
    "    score2 = 0 if score_list[index+1] < 16 else 1 if 15 < score_list[index+1] < 24 else 2 if 23 < score_list[index+1] < 32 else 3\n",
    "    score3 = 0 if score_list[index+2] < 16 else 1 if 15 < score_list[index+2] < 24 else 2 if 23 < score_list[index+2] < 32 else 3\n",
    "    return int((score1+score2+score3)/3)\n",
    "\n",
    "def create_Dataset(): #combine both of these functions to include all the propper columns\n",
    "    DATASET_CSV = \"./Dataset.csv\"\n",
    "    NUMPY_FILES_PATH = \"./Dataset/Train/numpy_files_reduced\"\n",
    "    header = { \"VIDEO\" : [], \n",
    "              \"NUMPY\": [], \n",
    "              \"GRS\" : [],}\n",
    "\n",
    "    df_dataset = pd.DataFrame(header)\n",
    "    for i in range(0,len(video_list),3):\n",
    "        row = []\n",
    "        row.append(video_list[i])#VIDEO name\n",
    "        row.append(f\"{NUMPY_FILES_PATH}/{video_list[i]}.npz\")#NUMPY file\n",
    "        row.append(get_GRS(rating_score, i)) #GRS\n",
    "        df_dataset.loc[len(df_dataset)] = row\n",
    "    df_dataset.to_csv(DATASET_CSV, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5e987-b1e5-4524-a11c-fd4912a31625",
   "metadata": {},
   "source": [
    "## Step 5 - Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f6269-db4d-4894-9c8f-05bc949c7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys = [\"NUMPY\"], reader = NumpyReader(npz_keys = (\"video\"))),\n",
    "    CastToTyped(keys = [\"NUMPY\"]), #cast to float32\n",
    "    ToTensor(),\n",
    "])            \n",
    "\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys = [\"NUMPY\"], reader = NumpyReader(npz_keys = (\"video\"))),\n",
    "    CastToTyped(keys = [\"NUMPY\"]), #cast to float32\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# create a training data loader\n",
    "train_ds = CSVDataset(src = \"./Dataset.csv\", col_names = [\"NUMPY\", \"GRS\"], row_indices = [[0,250]], transform = train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2, pin_memory = pin_memory, drop_last = True)\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = CSVDataset(src = \"./Dataset.csv\", col_names = [\"NUMPY\", \"GRS\"], row_indices = [[251,314]], transform = val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size = BATCH_SIZE, num_workers = 2, pin_memory = pin_memory, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0e360-d538-4f27-8298-7e75758d0658",
   "metadata": {},
   "source": [
    "## Step 6 - Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b925c-2e67-4c44-a287-049dbd4ad5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 300*300\n",
    "hidden_channels = 9000 #int(in_channels/2)\n",
    "\n",
    "# Definição classe para o modelo\n",
    "class MLP(Module):\n",
    "    # definir elementos do modelo\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input para a primeira camada\n",
    "        self.hidden1 = Linear(n_inputs, hidden_channels)\n",
    "        xavier_uniform_(self.hidden1.weight)  #O underscore significa que a inicialização vai ser feita por referencia e não pelo return\n",
    "        self.act1 = ReLU()\n",
    "        self.hidden2 = Linear(hidden_channels, 4)\n",
    "        xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = Softmax(dim = 1)\n",
    " \n",
    "    # sequencia de propagação do input \n",
    "    def forward(self, X):\n",
    "        # input para a primeira camada\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # segunda camada\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba4121-f79d-482e-9c63-3df1470ede4d",
   "metadata": {},
   "source": [
    "## Step 7 - Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8fc05-711a-48c6-b780-8565bda381e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(in_channels).to(device)\n",
    "\n",
    "BEST_MODEL_PATH = f\"./model/MLP\"\n",
    "BEST_MODEL_FILE = f\"{BEST_MODEL_PATH}/best_metric_model_classification3d_array_attempt{len(glob(f'{BEST_MODEL_PATH}/*'))+1:02}.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# treino do modelo\n",
    "def train_model(train_dl, model):\n",
    "    liveloss = PlotLosses() ##para visualizarmos o processo de treino\n",
    "    # definir o loss e a função de otimização\n",
    "    #criterion = torch.nn.MSELoss() #neste caso implementa a sparse_categorical_crossentropy\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    #optimizer = SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9) #stochastic gradient descent\n",
    "    optimizer = Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "    # iterar as epochs\n",
    "    #lowest_rmse = sys.float_info.max\n",
    "    highest_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        logs = {} ##para visualizarmos o processo de treino\n",
    "        # iterar as batchs\n",
    "        epoch_loss  = 0 ##para visualizarmos o processo de treino\n",
    "        #epoch_rmse  = 0 ##para visualizarmos o processo de treino\n",
    "        epoch_f1 = 0\n",
    "        for batch_data in train_dl:\n",
    "            inputs, labels = batch_data[\"NUMPY\"].to(device), batch_data[\"GRS\"].to(device)\n",
    "            # inicializar os gradientes\n",
    "            optimizer.zero_grad() #coloca os gradientes de todos os parametros a zero\n",
    "            # calcular o output do modelo\n",
    "            outputs = model(inputs.reshape(BATCH_SIZE, -1))\n",
    "            labels = torch.nn.functional.one_hot(labels.long(), num_classes = 4).float()\n",
    "            # calcular o loss\n",
    "            loss = criterion(outputs, labels) #.unsqueeze(1)\n",
    "            f1 = f1_score(torch.argmax(labels, dim = 1).cpu().detach().numpy().astype(int), torch.argmax(outputs,dim = 1).cpu().detach().numpy().astype(int), average = 'macro')\n",
    "            # atribuição alteraçoes \"In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "            #with respect to the output, and we need to compute the gradient of the loss with respect to the input.\n",
    "            loss.backward()\n",
    "            # update pesos do modelo\n",
    "            optimizer.step()\n",
    "            #só para multiclass:\n",
    "            #valores, predictions = torch.max(outputs, 1) #retorna um tensor com os indices do valor maximo em cada caso\n",
    "            epoch_loss += loss.item()\n",
    "            #epoch_rmse += rmse.item()\n",
    "            epoch_f1 += f1.item()\n",
    "\n",
    "        #print(f'Epoch {epoch:03}: | Loss: {epoch_loss/len(train_dl):.5f} | RMSE: {epoch_rmse/len(train_dl):.3f}')   \n",
    "        print(f'Epoch {epoch:03}: | Loss: {epoch_loss/len(train_dl):.5f} | F1 Score: {epoch_f1/len(train_dl):.3f}')      \n",
    "        logs['loss'] = epoch_loss/len(train_dl) ##para visualizarmos o processo de treino\n",
    "        #logs['rmse'] = epoch_rmse/len(train_dl) ##para visualizarmos o processo de treino\n",
    "        logs['f1_score'] = epoch_f1/len(train_dl) ##para visualizarmos o processo de treino\n",
    "        liveloss.update(logs) ##para visualizarmos o processo de treino\n",
    "        liveloss.send() ##para visualizarmos o processo de treino\n",
    "\n",
    "        if epoch_f1/len(train_dl) > highest_f1:\n",
    "            highest_f1 = f1\n",
    "            highest_f1_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), BEST_MODEL_FILE)\n",
    "            print(\"saved new best metric model\")\n",
    " \n",
    "# treinar o modelo\n",
    "#train_model(train_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48f1b0-7751-47b1-b0b8-67b54e06d2c3",
   "metadata": {},
   "source": [
    "## Step 8 - Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad10cf-2014-4d26-82be-09e33d86e787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Avaliar o modelo\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions = list()\n",
    "    actual_values = list()\n",
    "    for val_data in test_dl:\n",
    "        inputs, labels = val_data[\"NUMPY\"].to(device), val_data[\"GRS\"].to(device)\n",
    "        labels = torch.nn.functional.one_hot(labels.long(), num_classes = 4).float()\n",
    "        # avaliar o modelo com os casos de teste\n",
    "        yprev = model(inputs.reshape(BATCH_SIZE, -1))\n",
    "        # retirar o array numpy\n",
    "        \n",
    "        yprev = torch.argmax(yprev, dim = 1).cpu().detach().numpy().astype(int)\n",
    "        actual = torch.argmax(labels, dim = 1).cpu().detach().numpy().astype(int)\n",
    "        #actual = actual.reshape((len(actual), 1))\n",
    "        # guardar\n",
    "        predictions.append(yprev)\n",
    "        actual_values.append(actual)\n",
    "\n",
    "    predictions, actual_values = np.vstack(predictions), np.vstack(actual_values)\n",
    "    return actual_values, predictions\n",
    " \n",
    "\n",
    "#avaliar o modelo\n",
    "actual_values, predictions = evaluate_model(val_loader, model)\n",
    "#actuals, predictions = evaluate_model(train_dl, model)\n",
    "for r,p in zip(actual_values, predictions):\n",
    "    print(f'real:{r} previsão:{p}') \n",
    "\n",
    "#calcular a accuracy\n",
    "mse = mean_squared_error(actual_values, predictions)\n",
    "print(f'MSE: {mse:0.3f}, RMSE: {np.sqrt(mse):0.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3bda3-d313-4b55-a2d8-3b5479194527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(BEST_MODEL_FILE))\n",
    "all_outputs = np.array([])\n",
    "all_labels = np.array([])\n",
    "for val_data in val_loader:\n",
    "    val_images, val_labels = val_data[\"NUMPY\"].to(device), val_data[\"GRS\"].to(device)\n",
    "    val_labels = torch.nn.functional.one_hot(val_labels.long(), num_classes = 4).float()\n",
    "    with torch.no_grad():\n",
    "        val_prev = model(val_images.reshape(BATCH_SIZE, -1))\n",
    "        all_outputs = np.append(all_outputs, torch.argmax(val_prev, dim = 1).cpu().detach().numpy().astype(int))\n",
    "        all_labels = np.append(all_labels, torch.argmax(val_labels, dim = 1).cpu().detach().numpy().astype(int))\n",
    "\n",
    "for r,p in zip(all_labels, all_outputs):\n",
    "    print(f'real:{r} previsão:{p}') \n",
    "print(f\"F1-Score: {f1_score(all_labels, all_outputs, average = 'macro'):0.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
